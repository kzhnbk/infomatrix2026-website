{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSK Cancer Treatment Classification Pipeline\n",
    "\n",
    "## Complete Machine Learning Pipeline with BERT & Advanced Metrics\n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for **Memorial Sloan Kettering Cancer Center (MSK)** treatment classification. The pipeline includes:\n",
    "\n",
    "- **Text Preprocessing** with NLTK\n",
    "- **Feature Engineering** using TF-IDF and BERT embeddings\n",
    "- **Multiple ML Models** (Naive Bayes, Logistic Regression, Random Forest, XGBoost, LightGBM)\n",
    "- **Advanced Evaluation Metrics** for healthcare applications\n",
    "- **Production-ready code** optimized for Kaggle\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- Handles class imbalance with weighted sampling  \n",
    "- Multi-class classification with comprehensive metrics  \n",
    "- BERT-based deep learning embeddings  \n",
    "- Visualization of model performance  \n",
    "- Modular, reusable code structure  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Import Libraries & Dependencies\n",
    "\n",
    "Installing and importing all required packages for data processing, modeling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Core libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK data (quiet mode)\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"NLTK resources downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"NLTK download warning: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn: Preprocessing & Feature Engineering\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "print(\"Scikit-learn preprocessing modules loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn: Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    log_loss, \n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score\n",
    ")\n",
    "\n",
    "print(\"Evaluation metrics loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn: Machine Learning Models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"Scikit-learn models loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Advanced Gradient Boosting Libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"XGBoost loaded successfully!\")\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "    print(\"LightGBM loaded successfully!\")\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"LightGBM not available. Install with: pip install lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: BERT & Transformers for Deep Learning\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    BERT_AVAILABLE = True\n",
    "    print(\"Transformers & PyTorch loaded successfully!\")\n",
    "    print(f\"   PyTorch version: {torch.__version__}\")\n",
    "    print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "    print(\"Transformers not available. Install with: pip install transformers torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Loading & Exploration\n",
    "\n",
    "Load the MSK cancer treatment dataset and perform initial exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Replace with your actual data path\n",
    "DATA_PATH = '/kaggle/input/msk-cancer-treatment/training_variants.zip'\n",
    "\n",
    "# Example: Load from CSV\n",
    "# df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# For demonstration, create a sample dataset\n",
    "# Remove this section and load your actual data\n",
    "print(\"Loading dataset...\")\n",
    "print(\"Please replace this with your actual data loading code\")\n",
    "\n",
    "# Sample data structure (replace with actual loading)\n",
    "# df = pd.read_csv('your_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "# print(f\"\\nDataset Shape: {df.shape}\")\n",
    "# print(f\"\\nColumn Names:\\n{df.columns.tolist()}\")\n",
    "# print(f\"\\nFirst Few Rows:\")\n",
    "# display(df.head())\n",
    "# print(f\"\\nData Types:\")\n",
    "# print(df.dtypes)\n",
    "# print(f\"\\nMissing Values:\")\n",
    "# print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Text Preprocessing Pipeline\n",
    "\n",
    "Clean and normalize text data for optimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove URLs\n",
    "    3. Remove special characters\n",
    "    4. Remove extra whitespace\n",
    "    5. Remove stopwords\n",
    "    6. Tokenization\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw input text\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers (optional - depends on use case)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Tokenization and stopword removal\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "        text = ' '.join(tokens)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the function\n",
    "sample_text = \"This is a SAMPLE text with URLs http://example.com and numbers 123!\"\n",
    "cleaned = clean_text(sample_text)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned:  {cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning to your dataset\n",
    "# Example:\n",
    "# df['cleaned_text'] = df['text_column'].apply(clean_text)\n",
    "# print(\"Text preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Engineering\n",
    "\n",
    "### 4.1 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_features(texts, max_features=5000):\n",
    "    \"\"\"\n",
    "    Create TF-IDF features from text data\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text documents\n",
    "        max_features: Maximum number of features\n",
    "    \n",
    "    Returns:\n",
    "        X: TF-IDF feature matrix\n",
    "        vectorizer: Fitted TF-IDF vectorizer\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=(1, 2),  # unigrams and bigrams\n",
    "        min_df=2,            # minimum document frequency\n",
    "        max_df=0.95          # maximum document frequency\n",
    "    )\n",
    "    \n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    print(f\"TF-IDF Features Created\")\n",
    "    print(f\"   Shape: {X.shape}\")\n",
    "    print(f\"   Vocabulary Size: {len(vectorizer.vocabulary_)}\")\n",
    "    \n",
    "    return X, vectorizer\n",
    "\n",
    "# Example usage:\n",
    "# X_tfidf, tfidf_vectorizer = create_tfidf_features(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 BERT Embeddings (Optional - Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_embeddings(texts, model_name='bert-base-uncased', batch_size=16):\n",
    "    \"\"\"\n",
    "    Create BERT embeddings for text data\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text documents\n",
    "        model_name: HuggingFace model name\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: Numpy array of BERT embeddings\n",
    "    \"\"\"\n",
    "    if not BERT_AVAILABLE:\n",
    "        print(\"BERT not available. Skipping BERT embeddings.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading BERT model: {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            # Use [CLS] token embedding\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        embeddings.append(batch_embeddings)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"   Processed {i + len(batch_texts)}/{len(texts)} texts\")\n",
    "    \n",
    "    embeddings = np.vstack(embeddings)\n",
    "    print(f\"BERT Embeddings Created: {embeddings.shape}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Example usage:\n",
    "# if BERT_AVAILABLE:\n",
    "#     X_bert = create_bert_embeddings(df['cleaned_text'][:100])  # Test on subset first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Target Encoding & Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(df['target_column'])\n",
    "\n",
    "# print(f\"\\nTarget Classes: {label_encoder.classes_}\")\n",
    "# print(f\"Class Distribution:\")\n",
    "# print(pd.Series(y_encoded).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_tfidf, \n",
    "#     y_encoded, \n",
    "#     test_size=0.2, \n",
    "#     random_state=42,\n",
    "#     stratify=y_encoded  # Maintain class distribution\n",
    "# )\n",
    "\n",
    "# print(f\"\\nData Split Complete\")\n",
    "# print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "# print(f\"   Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Training Pipeline\n",
    "\n",
    "### 6.1 Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(y):\n",
    "    \"\"\"\n",
    "    Calculate class weights for imbalanced datasets\n",
    "    \n",
    "    Args:\n",
    "        y: Target labels\n",
    "    \n",
    "    Returns:\n",
    "        class_weights: Dictionary of class weights\n",
    "    \"\"\"\n",
    "    classes = np.unique(y)\n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=y\n",
    "    )\n",
    "    \n",
    "    class_weights = dict(zip(classes, weights))\n",
    "    \n",
    "    print(\"Class Weights:\")\n",
    "    for cls, weight in class_weights.items():\n",
    "        print(f\"   Class {cls}: {weight:.4f}\")\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Example:\n",
    "# class_weights = calculate_class_weights(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(X_train, y_train):\n",
    "    \"\"\"Train Multinomial Naive Bayes classifier\"\"\"\n",
    "    print(\"\\nTraining Naive Bayes...\")\n",
    "    model = MultinomialNB(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Naive Bayes training complete!\")\n",
    "    return model\n",
    "\n",
    "def train_logistic_regression(X_train, y_train, class_weights=None):\n",
    "    \"\"\"Train Logistic Regression classifier\"\"\"\n",
    "    print(\"\\nTraining Logistic Regression...\")\n",
    "    model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=class_weights,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Logistic Regression training complete!\")\n",
    "    return model\n",
    "\n",
    "def train_random_forest(X_train, y_train, class_weights=None):\n",
    "    \"\"\"Train Random Forest classifier\"\"\"\n",
    "    print(\"\\nTraining Random Forest...\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        class_weight=class_weights,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Random Forest training complete!\")\n",
    "    return model\n",
    "\n",
    "def train_xgboost(X_train, y_train, class_weights=None):\n",
    "    \"\"\"Train XGBoost classifier\"\"\"\n",
    "    if not XGBOOST_AVAILABLE:\n",
    "        print(\"XGBoost not available\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nTraining XGBoost...\")\n",
    "    \n",
    "    # Convert class weights to sample weights\n",
    "    if class_weights is not None:\n",
    "        sample_weights = np.array([class_weights[y] for y in y_train])\n",
    "    else:\n",
    "        sample_weights = None\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        tree_method='hist',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    print(\"XGBoost training complete!\")\n",
    "    return model\n",
    "\n",
    "def train_lightgbm(X_train, y_train, class_weights=None):\n",
    "    \"\"\"Train LightGBM classifier\"\"\"\n",
    "    if not LIGHTGBM_AVAILABLE:\n",
    "        print(\"LightGBM not available\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nTraining LightGBM...\")\n",
    "    \n",
    "    # Convert class weights to sample weights\n",
    "    if class_weights is not None:\n",
    "        sample_weights = np.array([class_weights[y] for y in y_train])\n",
    "    else:\n",
    "        sample_weights = None\n",
    "    \n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    print(\"LightGBM training complete!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store trained models\n",
    "# models = {}\n",
    "\n",
    "# Train Naive Bayes\n",
    "# models['Naive Bayes'] = train_naive_bayes(X_train, y_train)\n",
    "\n",
    "# Train Logistic Regression\n",
    "# models['Logistic Regression'] = train_logistic_regression(X_train, y_train, class_weights)\n",
    "\n",
    "# Train Random Forest\n",
    "# models['Random Forest'] = train_random_forest(X_train, y_train, class_weights)\n",
    "\n",
    "# Train XGBoost (if available)\n",
    "# if XGBOOST_AVAILABLE:\n",
    "#     models['XGBoost'] = train_xgboost(X_train, y_train, class_weights)\n",
    "\n",
    "# Train LightGBM (if available)\n",
    "# if LIGHTGBM_AVAILABLE:\n",
    "#     models['LightGBM'] = train_lightgbm(X_train, y_train, class_weights)\n",
    "\n",
    "# print(f\"\\nAll models trained! Total models: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Model Evaluation & Metrics\n",
    "\n",
    "### 7.1 Comprehensive Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        y_test: True labels\n",
    "        model_name: Name for display\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get probability predictions if available\n",
    "    try:\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        has_proba = True\n",
    "    except:\n",
    "        y_pred_proba = None\n",
    "        has_proba = False\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "        'precision_macro': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "        'precision_weighted': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'recall_macro': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "        'recall_weighted': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'f1_macro': f1_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "        'f1_weighted': f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "    }\n",
    "    \n",
    "    # Add log loss if probabilities available\n",
    "    if has_proba:\n",
    "        results['log_loss'] = log_loss(y_test, y_pred_proba)\n",
    "        \n",
    "        # Calculate ROC AUC for multi-class\n",
    "        try:\n",
    "            y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "            if y_test_bin.shape[1] > 1:\n",
    "                results['roc_auc_ovr'] = roc_auc_score(\n",
    "                    y_test_bin, y_pred_proba, \n",
    "                    average='macro', \n",
    "                    multi_class='ovr'\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"   Accuracy:              {results['accuracy']:.4f}\")\n",
    "    print(f\"   Balanced Accuracy:     {results['balanced_accuracy']:.4f}\")\n",
    "    print(f\"   Precision (Macro):     {results['precision_macro']:.4f}\")\n",
    "    print(f\"   Precision (Weighted):  {results['precision_weighted']:.4f}\")\n",
    "    print(f\"   Recall (Macro):        {results['recall_macro']:.4f}\")\n",
    "    print(f\"   Recall (Weighted):     {results['recall_weighted']:.4f}\")\n",
    "    print(f\"   F1-Score (Macro):      {results['f1_macro']:.4f}\")\n",
    "    print(f\"   F1-Score (Weighted):   {results['f1_weighted']:.4f}\")\n",
    "    \n",
    "    if has_proba:\n",
    "        print(f\"   Log Loss:              {results['log_loss']:.4f}\")\n",
    "        if 'roc_auc_ovr' in results:\n",
    "            print(f\"   ROC AUC (OvR):         {results['roc_auc_ovr']:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "    return results, y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store evaluation results\n",
    "# all_results = []\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     results, y_pred, y_pred_proba = evaluate_model(model, X_test, y_test, model_name)\n",
    "#     all_results.append(results)\n",
    "\n",
    "# Create results DataFrame\n",
    "# results_df = pd.DataFrame(all_results)\n",
    "# results_df = results_df.set_index('model_name')\n",
    "# print(\"\\nSummary of All Models:\")\n",
    "# display(results_df.style.highlight_max(axis=0, color='lightgreen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Visualization & Analysis\n",
    "\n",
    "### 8.1 Model Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(results_df):\n",
    "    \"\"\"\n",
    "    Create comprehensive model comparison visualizations\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Accuracy Metrics\n",
    "    metrics = ['accuracy', 'balanced_accuracy']\n",
    "    results_df[metrics].plot(kind='bar', ax=axes[0, 0], rot=45)\n",
    "    axes[0, 0].set_title('Accuracy Metrics')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].legend(['Accuracy', 'Balanced Accuracy'])\n",
    "    axes[0, 0].set_ylim([0, 1])\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Precision & Recall\n",
    "    metrics = ['precision_macro', 'recall_macro']\n",
    "    results_df[metrics].plot(kind='bar', ax=axes[0, 1], rot=45)\n",
    "    axes[0, 1].set_title('Precision & Recall (Macro)')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].legend(['Precision', 'Recall'])\n",
    "    axes[0, 1].set_ylim([0, 1])\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. F1 Scores\n",
    "    metrics = ['f1_macro', 'f1_weighted']\n",
    "    results_df[metrics].plot(kind='bar', ax=axes[1, 0], rot=45)\n",
    "    axes[1, 0].set_title('F1 Scores')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].legend(['F1 Macro', 'F1 Weighted'])\n",
    "    axes[1, 0].set_ylim([0, 1])\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Overall Ranking\n",
    "    ranking_metric = 'f1_weighted'\n",
    "    top_models = results_df[ranking_metric].sort_values(ascending=True)\n",
    "    top_models.plot(kind='barh', ax=axes[1, 1], color='skyblue')\n",
    "    axes[1, 1].set_title(f'Model Ranking by {ranking_metric.replace(\"_\", \" \").title()}')\n",
    "    axes[1, 1].set_xlabel('Score')\n",
    "    axes[1, 1].set_xlim([0, 1])\n",
    "    axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "# plot_model_comparison(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model_name=\"Model\", class_names=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with percentages\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        cm_percent, \n",
    "        annot=True, \n",
    "        fmt='.1f', \n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names if class_names else 'auto',\n",
    "        yticklabels=class_names if class_names else 'auto',\n",
    "        cbar_kws={'label': 'Percentage (%)'},\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "# best_model_name = results_df['f1_weighted'].idxmax()\n",
    "# best_model = models[best_model_name]\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# plot_confusion_matrix(y_test, y_pred, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Model Saving & Export\n",
    "\n",
    "Save the best performing model for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model(model, vectorizer, label_encoder, model_name=\"best_model\"):\n",
    "    \"\"\"\n",
    "    Save model and preprocessing objects\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{model_name}_{timestamp}.pkl\"\n",
    "    \n",
    "    # Save as a dictionary\n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'vectorizer': vectorizer,\n",
    "        'label_encoder': label_encoder,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_package, filename)\n",
    "    print(f\"Model saved to: {filename}\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Load saved model\n",
    "    \"\"\"\n",
    "    model_package = joblib.load(filename)\n",
    "    print(f\"Model loaded from: {filename}\")\n",
    "    \n",
    "    return model_package\n",
    "\n",
    "# Example:\n",
    "# best_model_name = results_df['f1_weighted'].idxmax()\n",
    "# best_model = models[best_model_name]\n",
    "# save_model(best_model, tfidf_vectorizer, label_encoder, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Prediction Pipeline\n",
    "\n",
    "Complete end-to-end prediction pipeline for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_treatment(text, model_package):\n",
    "    \"\"\"\n",
    "    Predict treatment class for new text\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        model_package: Dictionary with model, vectorizer, label_encoder\n",
    "    \n",
    "    Returns:\n",
    "        prediction: Predicted class\n",
    "        probabilities: Class probabilities\n",
    "    \"\"\"\n",
    "    # Extract components\n",
    "    model = model_package['model']\n",
    "    vectorizer = model_package['vectorizer']\n",
    "    label_encoder = model_package['label_encoder']\n",
    "    \n",
    "    # Preprocess text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    X = vectorizer.transform([cleaned_text])\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X)[0]\n",
    "    prediction = label_encoder.inverse_transform([y_pred])[0]\n",
    "    \n",
    "    # Get probabilities if available\n",
    "    try:\n",
    "        probabilities = model.predict_proba(X)[0]\n",
    "        prob_dict = dict(zip(label_encoder.classes_, probabilities))\n",
    "    except:\n",
    "        prob_dict = None\n",
    "    \n",
    "    return prediction, prob_dict\n",
    "\n",
    "# Example usage:\n",
    "# sample_text = \"Patient presents with stage III melanoma...\"\n",
    "# prediction, probabilities = predict_treatment(sample_text, model_package)\n",
    "# print(f\"\\nPredicted Treatment: {prediction}\")\n",
    "# if probabilities:\n",
    "#     print(\"\\nClass Probabilities:\")\n",
    "#     for cls, prob in sorted(probabilities.items(), key=lambda x: x[1], reverse=True):\n",
    "#         print(f\"   {cls}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:29:10.094549Z",
     "iopub.status.busy": "2026-01-27T12:29:10.094079Z",
     "iopub.status.idle": "2026-01-27T12:29:10.120856Z",
     "shell.execute_reply": "2026-01-27T12:29:10.120209Z",
     "shell.execute_reply.started": "2026-01-27T12:29:10.094459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Additional utility functions can be added here\n",
    "\n",
    "def print_model_info(model):\n",
    "    \"\"\"Print model parameters and configuration\"\"\"\n",
    "    print(f\"\\nModel Type: {type(model).__name__}\")\n",
    "    print(f\"Parameters: {model.get_params()}\")\n",
    "\n",
    "def export_predictions_to_csv(y_true, y_pred, filename=\"predictions.csv\"):\n",
    "    \"\"\"Export predictions to CSV file\"\"\"\n",
    "    results = pd.DataFrame({\n",
    "        'true_label': y_true,\n",
    "        'predicted_label': y_pred,\n",
    "        'correct': y_true == y_pred\n",
    "    })\n",
    "    results.to_csv(filename, index=False)\n",
    "    print(f\"Predictions saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 44307,
     "sourceId": 6841,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30301,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
